# train.py å®ç°è®¡åˆ’ (ç»­2)

## ä¸ƒã€ä½¿ç”¨ç¤ºä¾‹

### 7.1 åŸºç¡€ä½¿ç”¨

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python train.py

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
python train.py --config configs/my_config.json

# å¿«é€Ÿæµ‹è¯•æ¨¡å¼
python train.py --quick-test

# æŒ‡å®šå®éªŒåç§°
python train.py --experiment-name "exp_001"

# è®¾ç½®éšæœºç§å­ï¼ˆå¯å¤ç°ï¼‰
python train.py --seed 42
```

### 7.2 æ¢å¤è®­ç»ƒ

```bash
# ä»æ£€æŸ¥ç‚¹æ¢å¤
python train.py --resume checkpoints/exp_001/checkpoint_gen_25.pth

# ä»æœ€ä½³æ£€æŸ¥ç‚¹æ¢å¤
python train.py --resume checkpoints/exp_001/best_gen_30.pth
```

### 7.3 è‡ªå®šä¹‰é…ç½®

```bash
# è¦†ç›–ç‰¹å®šå‚æ•°
python train.py --population-size 16 --num-generations 100 --num-workers 8
```

### 7.4 é…ç½®æ–‡ä»¶ç¤ºä¾‹

åˆ›å»º `configs/default.json`:

```json
{
  "population_size": 8,
  "elite_size": 2,
  "tournament_size": 3,
  "crossover_alpha": 0.5,
  "mutation_rate": 0.1,
  "initial_temperature": 1.0,
  "min_temperature": 0.1,
  "cooling_rate": 0.95,
  "num_generations": 50,
  "max_game_steps": 1000,
  "action_temperature": 1.0,
  "round_robin_until": 10,
  "tournament_games": 4,
  "num_workers": 4,
  "d_model": 128,
  "num_layers": 2,
  "nhead": 4,
  "dim_feedforward": 256,
  "dropout": 0.1,
  "checkpoint_dir": "checkpoints",
  "save_every": 5,
  "keep_best_n": 3,
  "log_dir": "logs",
  "log_every": 1,
  "seed": 42,
  "experiment_name": "ctf_evolution"
}
```

---

## å…«ã€è¾“å‡ºç¤ºä¾‹

### 8.1 æ§åˆ¶å°è¾“å‡º

```
============================================================
å¼€å§‹è¿›åŒ–è®­ç»ƒ
============================================================
é…ç½®: TrainingConfig(population_size=8, num_generations=50, ...)
ç§ç¾¤å·²åˆ›å»º: 8 ä¸ªä½“

============================================================
ä¸–ä»£ 0/50
æ¸©åº¦: 1.0000
============================================================
ä¸–ä»£ 0: åˆ›å»º 28 åœºå¯¹æˆ˜
å¼€å§‹æ‰§è¡Œ 28 åœºå¯¹æˆ˜ (å¹¶è¡Œåº¦: 4)
è¿›åº¦: 28/28 (100.0%)
å¼€å§‹é—ä¼ æ¼”åŒ–...
âœ“ é—ä¼ æ¼”åŒ–å®Œæˆ
[Gen   0] T=1.000 | Games= 28 | Fitness: 120.45 /  85.32 /  45.20 | Wins: L= 15 R= 12 D=  1 | Time: 45.2s
âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜: checkpoints/ctf_evolution/best_gen_0.pth

============================================================
ä¸–ä»£ 1/50
æ¸©åº¦: 0.9500
============================================================
...
```

### 8.2 æ—¥å¿—æ–‡ä»¶

**training.log**:
```
[2025-12-28 10:30:15] ç§ç¾¤å·²åˆ›å»º: 8 ä¸ªä½“
[Gen   0] T=1.000 | Games= 28 | Fitness: 120.45 /  85.32 /  45.20 | Wins: L= 15 R= 12 D=  1 | Time: 45.2s
[Gen   1] T=0.950 | Games= 28 | Fitness: 135.20 /  92.15 /  50.30 | Wins: L= 16 R= 11 D=  1 | Time: 90.5s
...
```

**training_log.csv**:
```csv
generation,timestamp,temperature,num_games,avg_steps,avg_duration_ms,best_fitness,avg_fitness,worst_fitness,l_wins,r_wins,draws
0,2025-12-28 10:30:15,1.0000,28,450.5,15234.2,120.45,85.32,45.20,15,12,1
1,2025-12-28 10:31:00,0.9500,28,465.3,15890.1,135.20,92.15,50.30,16,11,1
...
```

---

## ä¹ã€å®ç°æ³¨æ„äº‹é¡¹

### 9.1 å†…å­˜ç®¡ç†

âš ï¸ **é—®é¢˜**ï¼šé•¿æ—¶é—´è®­ç»ƒå¯èƒ½å¯¼è‡´å†…å­˜æ³„æ¼

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
import gc
import torch

# åœ¨æ¯ä¸ªä¸–ä»£ç»“æŸå
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
```

### 9.2 å¼‚å¸¸å¤„ç†

âš ï¸ **é‡è¦**ï¼šç¡®ä¿å¼‚å¸¸æ—¶ä¿å­˜æ£€æŸ¥ç‚¹

```python
try:
    trainer.train()
except KeyboardInterrupt:
    # ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜ç´§æ€¥æ£€æŸ¥ç‚¹
    checkpoint_manager.save_checkpoint(
        current_generation,
        population,
        temperature,
        stats,
        is_best=False
    )
except Exception as e:
    # å…¶ä»–å¼‚å¸¸ï¼Œè®°å½•å¹¶ä¿å­˜
    logger.log_message(f"é”™è¯¯: {e}")
    checkpoint_manager.save_checkpoint(...)
    raise
```

### 9.3 è¿›åº¦ç›‘æ§

âš ï¸ **å»ºè®®**ï¼šä½¿ç”¨ `tqdm` æ˜¾ç¤ºè¿›åº¦æ¡

```python
from tqdm import tqdm

for generation in tqdm(range(num_generations), desc="è®­ç»ƒè¿›åº¦"):
    ...
```

### 9.4 åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¯é€‰ï¼‰

âš ï¸ **é«˜çº§**ï¼šæ”¯æŒå¤šæœºè®­ç»ƒ

```python
# ä½¿ç”¨ Ray æˆ– Dask è¿›è¡Œåˆ†å¸ƒå¼è®¡ç®—
import ray

@ray.remote
def run_game_remote(ind_l, ind_r, ...):
    return run_single_game(ind_l, ind_r, ...)

# å¹¶è¡Œæ‰§è¡Œ
futures = [run_game_remote.remote(...) for matchup in matchups]
results = ray.get(futures)
```

---

## åã€æµ‹è¯•ä»£ç 

### 10.1 test_training_config

```python
def test_training_config():
    """æµ‹è¯•é…ç½®ç®¡ç†"""
    # åˆ›å»ºé…ç½®
    config = TrainingConfig(
        population_size=4,
        num_generations=10,
        experiment_name="test"
    )

    # ä¿å­˜é…ç½®
    save_config(config, "test_config.json")

    # åŠ è½½é…ç½®
    loaded_config = load_config("test_config.json")

    assert loaded_config.population_size == 4
    assert loaded_config.num_generations == 10

    print("âœ“ é…ç½®ç®¡ç†æµ‹è¯•é€šè¿‡")
```

### 10.2 test_checkpoint_manager

```python
def test_checkpoint_manager():
    """æµ‹è¯•æ£€æŸ¥ç‚¹ç®¡ç†"""
    from population import Population, PopulationConfig
    from transformer_model import CTFTransformerConfig

    # åˆ›å»ºç§ç¾¤
    pop_config = PopulationConfig(population_size=4)
    model_config = CTFTransformerConfig(d_model=64, num_layers=1)
    population = Population(pop_config, model_config)

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨
    manager = CheckpointManager("test_checkpoints", keep_best_n=2)

    # ä¿å­˜æ£€æŸ¥ç‚¹
    stats = {'best_fitness': 100.0, 'avg_fitness': 80.0}
    path = manager.save_checkpoint(0, population, 1.0, stats, is_best=True)

    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = manager.load_checkpoint(path)

    assert checkpoint['generation'] == 0
    assert checkpoint['stats']['best_fitness'] == 100.0

    print("âœ“ æ£€æŸ¥ç‚¹ç®¡ç†æµ‹è¯•é€šè¿‡")
```

### 10.3 test_full_training

```python
def test_full_training():
    """æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆå°è§„æ¨¡ï¼‰"""
    config = TrainingConfig(
        population_size=4,
        num_generations=2,
        max_game_steps=50,
        num_workers=2,
        experiment_name="test_training"
    )

    trainer = EvolutionaryTrainer(config)

    try:
        trainer.train()
        print("âœ“ å®Œæ•´è®­ç»ƒæµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"âœ— è®­ç»ƒå¤±è´¥: {e}")
        raise
```

---

## åä¸€ã€æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 11.1 å¹¶è¡Œåº¦è°ƒä¼˜

```python
# æ ¹æ®CPUæ ¸å¿ƒæ•°è‡ªåŠ¨è®¾ç½®
import os
num_workers = max(1, os.cpu_count() - 2)
```

### 11.2 æ‰¹é‡å¤„ç†

```python
# æ‰¹é‡æ‰§è¡Œæ¸¸æˆï¼Œå‡å°‘è¿›ç¨‹åˆ›å»ºå¼€é”€
batch_size = 10
for i in range(0, len(matchups), batch_size):
    batch = matchups[i:i+batch_size]
    results.extend(executor.execute_matchups(batch))
```

### 11.3 æ¨¡å‹ç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼‰

```python
# ä½¿ç”¨ torch.compile åŠ é€Ÿæ¨ç†
if hasattr(torch, 'compile'):
    model = torch.compile(model)
```

---

## åäºŒã€æ–‡ä»¶ç»“æ„

```
train.py
â”œâ”€â”€ TrainingConfig (æ•°æ®ç±»)
â”œâ”€â”€ load_config / save_config (å‡½æ•°)
â”œâ”€â”€ CheckpointManager (ç±»)
â”œâ”€â”€ TrainingLogger (ç±»)
â”œâ”€â”€ EvolutionaryTrainer (ä¸»ç±»)
â”œâ”€â”€ parse_arguments (å‡½æ•°)
â”œâ”€â”€ main (å‡½æ•°)
â””â”€â”€ æµ‹è¯•å‡½æ•°
```

---

## åä¸‰ã€é¢„ä¼°ä»£ç é‡

- TrainingConfig: ~50è¡Œ
- é…ç½®ç®¡ç†: ~40è¡Œ
- CheckpointManager: ~120è¡Œ
- TrainingLogger: ~100è¡Œ
- EvolutionaryTrainer: ~200è¡Œ
- å‘½ä»¤è¡Œæ¥å£: ~80è¡Œ
- æµ‹è¯•ä»£ç : ~100è¡Œ

**æ€»è®¡**: ~690è¡Œ

---

## åå››ã€ä¾èµ–å…³ç³»

```
train.py
â”œâ”€â”€ population.py (Population, PopulationConfig)
â”œâ”€â”€ transformer_model.py (CTFTransformer, CTFTransformerConfig)
â”œâ”€â”€ reward_system.py (AdaptiveRewardSystem)
â”œâ”€â”€ adversarial_trainer.py (AdversarialTrainer, AdaptiveMatchupStrategy)
â”œâ”€â”€ genetic_ops.py (evolve_generation, AnnealingScheduler)
â””â”€â”€ æ ‡å‡†åº“ (argparse, json, csv, pathlib, etc.)
```

**æ‰€æœ‰ä¾èµ–å·²å®ç°ï¼Œå¯ç›´æ¥å¼€å‘ã€‚**

---

## åäº”ã€å®ç°é¡ºåºå»ºè®®

1. **ç¬¬ä¸€æ­¥**: TrainingConfig + é…ç½®ç®¡ç†ï¼ˆç®€å•ï¼‰
2. **ç¬¬äºŒæ­¥**: CheckpointManagerï¼ˆæ ¸å¿ƒï¼‰
3. **ç¬¬ä¸‰æ­¥**: TrainingLoggerï¼ˆæ ¸å¿ƒï¼‰
4. **ç¬¬å››æ­¥**: EvolutionaryTrainerï¼ˆä¸»ç±»ï¼‰
5. **ç¬¬äº”æ­¥**: å‘½ä»¤è¡Œæ¥å£ï¼ˆç®€å•ï¼‰
6. **ç¬¬å…­æ­¥**: æµ‹è¯•å¹¶éªŒè¯

---

## åå…­ã€é¢„æœŸè®­ç»ƒæ—¶é—´

### 16.1 å•åœºæ¸¸æˆ

- å¹³å‡æ­¥æ•°: 500æ­¥
- å•æ­¥è€—æ—¶: ~30ms
- å•åœºè€—æ—¶: ~15ç§’

### 16.2 å•ä¸–ä»£

- ç§ç¾¤å¤§å°: 8
- å¾ªç¯èµ›: 28åœº
- å¹¶è¡Œåº¦: 4
- ä¸–ä»£è€—æ—¶: ~105ç§’ï¼ˆ1.75åˆ†é’Ÿï¼‰

### 16.3 å®Œæ•´è®­ç»ƒ

- ä¸–ä»£æ•°: 50
- æ€»è€—æ—¶: ~88åˆ†é’Ÿï¼ˆ1.5å°æ—¶ï¼‰
- åŠ é€Ÿå: ~30-40åˆ†é’Ÿï¼ˆä¼˜åŒ–å¹¶è¡Œåº¦ï¼‰

---

## åä¸ƒã€åç»­æ‰©å±•

### 17.1 å¯è§†åŒ–

```python
# ä½¿ç”¨ matplotlib ç»˜åˆ¶è®­ç»ƒæ›²çº¿
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv("logs/ctf_evolution/training_log.csv")
plt.plot(df['generation'], df['best_fitness'], label='Best')
plt.plot(df['generation'], df['avg_fitness'], label='Average')
plt.xlabel('Generation')
plt.ylabel('Fitness')
plt.legend()
plt.savefig('training_curve.png')
```

### 17.2 è¶…å‚æ•°æœç´¢

```python
# ä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
import optuna

def objective(trial):
    config = TrainingConfig(
        mutation_rate=trial.suggest_float('mutation_rate', 0.05, 0.2),
        cooling_rate=trial.suggest_float('cooling_rate', 0.90, 0.98),
        ...
    )
    trainer = EvolutionaryTrainer(config)
    trainer.train()
    return trainer.best_fitness_ever

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)
```

### 17.3 å¯¹æˆ˜å¯è§†åŒ–

```python
# ä¿å­˜å¯¹æˆ˜å›æ”¾
def save_game_replay(episode_result, filepath):
    replay = {
        'trajectory': episode_result.trajectory,
        'winner': episode_result.winner,
        'scores': (episode_result.l_score, episode_result.r_score)
    }
    with open(filepath, 'w') as f:
        json.dump(replay, f)
```

---

**å®ç°æ—¥æœŸ**: 2025-12-28
**çŠ¶æ€**: ğŸ“‹ å¾…å®ç°
